\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvm}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{stfloats}
\usepackage{setspace}

\usepackage{color}

\newcommand{\cxj}[1]{\textcolor{red}{(Cxj: #1)}}
\newcommand{\nt}[1]{\textcolor{blue}{(#1)}}
\newcommand{\amend}[1]{\textcolor{green}{(#1)}}

%数学命令
%\input{math-commands.tex}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


% \cvmfinalcopy % *** Uncomment this line for the final submission

\def\cvmPaperID{196} % *** Enter the cvm Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvmfinal\pagestyle{empty}\fi

\graphicspath{{figures/}}  % 图片存放路径

\begin{document}

%%%%%%%%% TITLE
\title{Effect of Instance Normalization on Fine-Grained Control for Sketch-Based Face Image Generation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\small\url{http://www.author.org/~second}}
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
	Sketching is an intuitive and effective way for content creation. While significant progress has been made for photorealistic image generation by using generative adversarial networks, it remains challenging to control the synthetic content with fine-grained control.  
	The instance normalization layer, which is widely adopted in existing image translation networks, washes away details in the input sketch and leads to loss of precise control on the desired shape of the generated face images.
	In this paper, we comprehensively investigate the effect of instance normalization on generating photorealistic face images from hand-drawn sketches.
	We first introduce a visualization approach to analyze the feature embedding for sketches with a group of specific changes.  
	Based on the visual analysis, we modify the instance normalization layers in the baseline image translation model. 
	We designed a new set of hand-drawn sketches with 11 categories of specially designed changes and conducted extensive experimental analysis.  
	The results and user studies demonstrate that our method markedly improve the quality of synthesized images and the conformance with user intention.    
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Photorealistic face image synthesis from hand-drawn sketches has drawn a lot of attention in computer graphics and computer vision for many years. Typical approaches use generative and adversarial networks~(GANs)\cite{gan}, that  stack convolutional, normalization and nonlinearity layers as generators. Normalization layers normalize the parameter distribution in order to alleviate the issue of slow convergence in gradient update process and avoid gradient vanishing and gradient exploding,  which is vastly important in GANs.

Many normalization layers have been developed in recent GANs for various goals, such as batch normalization~\cite{bn}, group normalization~\cite{gn}, layer normalization~\cite{ln} and instance normalization~\cite{instance_norm}. 
Batch normalization~\cite{bn} eliminates the influence of internal covariate shift, effectively avoids the possible problems of gradient vanishing and gradient exploding in the process of gradient backpropagation, and speeds up the training time. 
Group normalization~\cite{gn} organizes the channels of a layer into different groups, and computes the mean and standard deviation within each group independently for normalization. It is independent of batch size, thus it is frequently used in tasks which prefers small mini-batch size, such as object detection and video classification.
Layer normalization~\cite{ln} computes the mean and variance used for normalization over all the channels of a single layer. It is more suitable for recurrent neural networks. Unlike batch normalization, layer normalization~\cite{instance_norm} performs exactly the same computation at training and test times.

Instance normalization is similar to layer normalization but goes one step further. It computes the mean and variance for normalization over \emph{each} channel in \emph{each} training example.
Recent studies show that instance normalization performs well on visual tasks such as style transfer and image translation~\cite{pix2pixhd,spade,cyclegan} when replacing batch normalization in GANs architecture. 
Nonetheless, instance normalization layers tend to wash away detailed information conveyed by the input sketches, thus it results in descent of the feature expression ability and imprecise control on face generation. 

It is essential to support fine-grained control in sketch-based content creation. 
We comprehensively investigate the effect of instance normalization on fine-grained control in sketch-based photorealistic face image generation using data visualization methods. 
We utilize principal component analysis (PCA)~\cite{pca} to visualize and analyze features extracted by the generator from sketches. 
Consequently, we propose to remove the first two instance normalization layers in the baseline generator, and show that this simple modification in the generator results in a significant improvement on control accuracy in image generation. 
We conduct experiments and interactive user studies to evaluate our proposed method, and the results demonstrate that our method surpasses the baseline method on image quality and control precision on the sketch-to-image task.

\section{Related Work}
\subsection{Image-to-Image Translation}
Image-to-image translation aims to convert an input image from one domain to another given the input-output image pairs as training data, in other words, to generate corresponding image according to the input image while the two images share the same scene structure. At present, many researchers employ adversarial manner to train deep neural networks in image-to-image translation tasks~\cite{cyclegan,bicyclegan,spagan,munit,crn,cfgan,sis,cfgan,maskgan}.

The concept of image-to-image translation was first proposed by pix2pix~\cite{pix2pix}, derived from generative adversarial networks while conditioned on images. 
The pix2pix network consists of a generator $G$ and a discriminator $D$. The generator converts the input image from a source domain to a target domain, and the discriminator tells the generated images apart from real images. 
This model can be applied to a variety of image translation scenarios, such as lable maps to streetscapes, edge maps to photos, image colorization, and so on.
However, the original pix2pix model has limits of low resolution images, at most $256 \times 256$. 
When pix2pix is applied to generate images with a higher resolution, the training process will be unstable and the generation quality will decline dramatically.
In order to improve the resolution of synthesized images, a subsequent model, pix2pixHD~\cite{pix2pixhd}, is proposed to images from semantic lable maps by a coarse-to-fine generator and a multiscale discriminator. 
However, the instance normalization layers used in pix2pixHD tend to wash away semantic information.
In order to efficiently preserve and propagate semantic information throughout the network, GauGAN~\cite{spade} utilizes semantic segmentation masks to modulate activations in the normalization layers through a spatially-adaptive and learned transformation. 
%
These models can also be applied to edge-to-photo generation when conditioned on edge map and photo pairs. 
However, the large gap between synthesized edge maps and hand-drawn sketches challenges the generalization ability of these models.
%
It inspires us to investigate the effect of normalization layers more deeply on information propagation in the network architecture for sketch-based face image synthesis.

\subsection{Normalization Layers}
In current deep neural networks, normalization layers play an important role for stabilizing the training process. 
We introduce several common normalization methods in detail. 
Batch normalization~\cite{bn} is a method that normalizes activations in a network across the mini-batch. 
It calculates the mean and variance among one channel over each mini-batch. Then, it learns two parameters to scale and shift the normalized activations.
Batch Normalization provides a strong way to reduce internal covariant shift problem and speed up the training process.
Group normalization~\cite{gn} divides the channels of activations into groups and then calculates the mean and standard deviation over the channel groups of each training sample for normalization.
Group normalization is frequently adopted in tasks such as object detection, semantic segmentation, and video classification. It helps deep learning models work better at small mini-batch size.
Layer normalization~\cite{ln} computes the mean and variance used for normalization over all the channels of a single layer. It is more suitable for recurrent neural networks. 
Unlike batch normalization, layer normalization performs exactly the same computation at training and test times.
Instance normalization~\cite{instance_norm} is similar to batch normalization. The only difference is that batch normalization computes the mean and variance among a mimi-batch, while instance normalization operates across only one channel of a single layer. 
Instance normalization performs well on style transfer~\cite{carigan,apdrawinggan,stylization,cartoongan,singlegan,transgaga,harmonic} tasks when replacing batch normalization.


\section{Feature Embedding Visualization}
We investigate the effect of instance normalization on fine-grained control in sketch-based photorealistic image generation by designing a set of hand-drawn sketches and visualizing the feature embedding.
In Sec.~\ref{sec:pix2pixhd}, we review our baseline method pix2pixHD.
In Sec.~\ref{sec:visualize}, we visualize the feature embedding from hand-drawn sketches by the generator of pix2pixHD, and analyze the visualization results to investigate the effect of instance normalization. 
Based on the analysis, we introduce our design on the generator network for sketch-based face generation in Sec.~\ref{sec:network}.

\subsection{The pix2pixHD Baseline}\label{sec:pix2pixhd}
Pix2pixHD~\cite{pix2pixhd} is an image translation model based on conditional GAN.
It adopts an improved adversarial loss and the network architecture to generate high-quality and high-resolution images from input semantic label maps. 
A coarse-to-fine generator and a multiscale discriminator are introduced to increase the image resolution and enhance texture details.
Using this model, more realistic images in dimension of $2048 \times 1024$ can be generated. 
%In addition, the model can also be used for interactive image editing.  

Its generator $G$ is composed of two sub-networks, a global generator network $G_1$ and a local enhance network $G_2$. 
$G_1$ is used to generate a base image, then $G_2$ is used to increase the image resolution with texture details. 
In order to distinguish real and generated images with high resolution, the discriminator requires a larger receptive field. 
Therefore, pix2pixHD uses a multi-scale discriminator, which is composed of three discriminators $D_1$, $D_2$ and $D_3$ in three scales, to preserve both global and local information. 
The loss function of this model is composed of three parts: adversarial loss $L_{GAN}(G,D_k)$, feature matching loss $L_{FM}(G,D_k)$, and VGG perceptual loss $L_{VGG}(G)$. The full objective is formulated as:
\begin{equation}
\begin{split}
	&\underset{G}{\min}\Bigg(\bigg(\underset{D_1,D_2,D_3}{\max} \sum_{k=1,2,3}{L_{GAN}\big(G,D_k\big)}\bigg)+  \\
	&\lambda \bigg(\frac{1}{3}\sum_{k=1,2,3}{L_{FM}(G,D_k)}+L_{VGG}{\big(G(\boldsymbol{s})\big)}\bigg)\Bigg),
\end{split}
\end{equation}
\noindent
where $\lambda$ controls the importance of the three terms.
 

Pix2pixHD can be applied to sketch-based face generation when trained using pairs of synthesized contours and photos. 
We developed an interactive system to support users to create face images by sketching. 
However, when an user tries to change the shape of a local part, pix2pixHD tends to change the entire image globally, as shown in Fig.~\ref{fig:problem-in}.
Therefore, we investigate the network architecture and feature embedding through a comprehensive experiment.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figures/prob-in-pix2pixHD.png}
	\caption{Pix2pixHD fails to support fine-grained control on the synthesized images when the input sketch changes locally at the mouth shape.}
	\label{fig:problem-in}
\end{figure}


\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\textwidth]{hand_drawn_sketches.png}
	\caption{We collect 11 groups of hand-drawn sketches with specific changes at each group. G1: Add hair; G2: Add new attributes, such as whiskers, wrinkles, and ears; G3: Face shape change; G4: Eyebrow change; G5: Eye shape change; G6: Eye size change; G7: Graffiti-drawn; G8: Mouth shape change; G9: Nose shape change; G10: Mouth shape change with the same eyes as G9; G11: Nose shape change with the same eyes as G8. Specifically, there is no correlation between G7 and the other 10 groups. Except G7, in the sketches of other groups, only a particular area or attribute is modified while the rest sketches in this group remain the same. While sketches in G8 and G11 have the same eyes, sketches in G9 and G10 have the same eyes, the eye shapes in other groups are different.}
	\label{fig:hand_drawn_contours}
\end{figure*}

\begin{figure}[htbp]
	\begin{center}
		%\centering
		\includegraphics[width=0.2 \textwidth]{receptive_field.png}
	\end{center}
	\caption{The receptive fields of corresponding left eye corner points in the feature maps of $L_0\sim L_4$. }
	\label{fig:receptive}
\end{figure}



\subsection{Feature Visualization}\label{sec:visualize}


%We extract the feature vectors of the middle layers of the generator network and utilize data reduction and visualization tool PCA to analyze them comparatively.

In order to analyze how the image translation model extracts the face shape features consistent with the user's intention for the hand-drawn sketches that typically have little details and geometric deformation, we collect 11 groups of sketches with changes in a specific area in each group. It contains 198 sketches totally of resolution $512\times512$. Fig.~\ref{fig:hand_drawn_contours} shows some examples of the 11 groups of sketches. And we refer to the 11 groups as $G1,\ldots,G11$.

We refer to the first five layers of the global generator of pix2pixHD as $L_0,\ldots,L_4$. 
With a sketch fed into the generator, five groups of feature maps can be obtained from $L_0$ to $L_4$.
For each point in a layer, we can extract a $k$-dimensional feature vector, where $k$ is the channel number of the feature maps in one layer.
We select the corner of the left eye for visual analysis. 
The five feature vectors are denoted as $\boldsymbol{v}_0,\boldsymbol{v}_1,\boldsymbol{v}_2,\boldsymbol{v}_3,\boldsymbol{v}_4$ in dimensions of 48, 96, 192, 384 and 768, respectively. 
%The coordinates of the corresponding points on the five feature maps are (170, 250), (85, 125), (43, 63), (22, 32) and (11, 16). 
The receptive fields of the five vectors are $7\times7$, $9\times9$, $13\times13$, $21\times21$, and $37\times37$ respectively on the input sketch, as shown in Fig.~\ref{fig:receptive}. 

%\textcolor{blue}{
%We get the five feature vectors of pix2pixHD generator called $\boldsymbol{v}_0^{'},\ldots,\boldsymbol{v}_4^{'}$ in the same way.}

 
\paragraph{Visualization with PCA.} Principal component analysis~(PCA)~\cite{pca} is a widely-used dimension reduction method and retains the statistic characteristics of data in high dimensional space. 
We employ PCA on $\boldsymbol{v}_0,\ldots, \boldsymbol{v}_4$ to map the high-dimensional vectors into 2D vectors and plot them in a 2D space. 
Fig.~\ref{fig:pca_0} show the visualization results with PCA on $\boldsymbol{v}_0,\ldots,\boldsymbol{v}_4$. 
%
In group 1, all the sketches share the same eye shape but with different hair styles. They are supposed to have the same feature embedding for the left eye corner at the early convolutional layers. However, as shown in Fig.~\ref{fig:pca_0}, the feature vectors scatter across a wide range (red numbers). 
Similarly, though the eye strokes do not change in the sketches of the same group, the feature embedding of G2 (\cxj{color}), G3 (\cxj{color}), and G4 \nt{color} scatter.
%
G8 and G11 scatter within group among a same area, so do the data of G9 and G10, whereas the data of G5, G6 and G7 dispersed globally.
\cxj{It is hard to follow here.}
This phenomenon indicates that the instance normalization in each layer takes the sketch information outside of its corresponding receptive field into account and then affects the feature embedding of a local region. 
By comparing visualization results of $\boldsymbol{v}_0\sim\boldsymbol{v}_4$ comprehensively, we find out that the feature vectors belonging to the groups without eye changes, such as G1, G2, G3, G4, G8, G9, G10, and G11, distribute more and more dispersedly within each group from layer 1 to layer 4, indicating that with more instance normalization, changing other parts on the input sketch has more and more influence on the loal feature embedding, resulting in an awful change of eyes in the generated images.
Consequently, we modify the normalization in the generator of pix2pixHD to keep local shape details. 

\begin{figure*}[htb]
	\centering
	\includegraphics[width=0.9 \textwidth]{pca_0.png}
	\caption{Visualization of feature embedding $\boldsymbol{v}_0\sim\boldsymbol{v}_4$ in the first five layers of the global generator of pix2pixHD for the left eye corner for 11 groups of sketches.  }
	\label{fig:pca_0}
\end{figure*}      



\section{Our Network Design}\label{sec:network}
%
With instance normalization in the baseline generator, the activation of convolutional output is normalized in a channel-wise manner and then modulated with unified scale and bias within each channel. 
This operation, to a certain extent, leads to a negative effect that a local change in the input sketch broadcasts globally, resulting in a degraded capacity of fine-grained control on the generated images.
However, the vanishing gradient or exploding gradient problem is bound to emerge when the instance normalization layers in the feature embedding stage of the generator is abandoned, making the training process difficult to converge. 
 
Based on the consideration above, we only remove the first two instance normalization layers in the global generator of pix2pixHD and keep the rest the same as pix2pixHD. 
The architecture of our generator is shown in Fig.~\ref{fig:our_generator}. 
%It operates at a resolution of 512×512. 
It consists of four components: a convolutional front-end without normalization $G_1^{(F)}$, a down-sampled convolutional mid-end $G_1^{(M)}$, a set of residual blocks $G_1^{(R)}$ and a transposed convolutional back-end $G_1^{(B)}$. 
\cxj{mark in the figure with the same symbols.}
$G_1^{(F)}$, which is composed of two unnormalized convolution and activation layers,  embeds local shape information of input sketches locally.
In other words, the local shape of the sketch can flow through the network without spatially broadcasting, thus effectively supports fine-grained control on generated images. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\columnwidth]{our_model_G.png}
	\caption{Network architecture of our generator. }
	\label{fig:our_generator}
\end{figure}

We use the same three-scale discriminator with pix2pixHD. 
Each discriminator is built on the PatchGAN~\cite{pix2pix} architecture. 
At each scale, the input sketch is concatenated with the corresponding face images, resized and fed into the corresponding discriminator.
 
 
To evaluate the superiority of our proposed method at the level of feature embedding, we conducted the visual analysis in the same way described in Sec.~\ref{sec:visualize}.
%
We refer to the five feature vectors in our generator as $\boldsymbol{v}_0^{'},\boldsymbol{v}_1^{'},\boldsymbol{v}_2^{'},\boldsymbol{v}_3^{'},\boldsymbol{v}_4^{'}$.
%
Fig.~\ref{fig:pca_1}(a) and Fig.~\ref{fig:pca_1}(b) demonstrate the results of PCA visualization on $\boldsymbol{v}_0^{'}$ and $\boldsymbol{v}_1^{'}$. 
It can be seen that the features of G1, G2, G3 and G4 are distributed at the same point separately, the data of G8 and G11 are gathered on the same point, the features of G9 and G10 are distributed at the same point, on the contrary, the data of G5, G6 and G7 are distributed dispersedly over the drawing space. 
This indicates that after the removal of instance normalization in the first two layers of generator, the feature embedding of the left eye corner conforms with the local shape of the input sketches inside of the corresponding receptive fields. The sketch change in the receptive field will influence the embedded features of the corresponding point but not outside of the receptive field.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.47 \textwidth]{pca_1.png}
	\caption{Results of PCA visualization on $\boldsymbol{v}_0^{'}$ and $\boldsymbol{v}_1^{'}$}
	\label{fig:pca_1}
\end{figure}

%Visualization results of $\boldsymbol{v}_2\sim\boldsymbol{v}_4$ are shown in Fig.\ref{fig:pca_2}(a)(b)(c). 
%By observing visualization results of $\boldsymbol{v}_2$, we can find out that the feature vectors belonging to categories with unmodified eyes have a different spatial distribution within class, which indicates that instance normalization conveys modification in other areas to left eye corner of sketches resulting in an effect on features extracted from left eye corner point. 
%\begin{figure*}[htb]
%	\centering
%	\includegraphics[width=0.8 \textwidth]{pca_2.png}
%	\caption{PCA visualization results of $\boldsymbol{v}_2 \sim \boldsymbol{v}_4$}
%	\label{fig:pca_2}
%\end{figure*}
%By comparing visualization results of $\boldsymbol{v}_2$ and $\boldsymbol{v}_4$, we can find out that the feature vectors belonging to categories with unmodified eyes such as G1, G2, G3, G4, G8, G9, G10, G11, distribute more and more dispersedly within class, which indicates that with the instance normalization operating layer by layer, changing other parts on the input sketch has more and more obvious influence on the features of the eye position, leading to the change of eyes in the generated image.

Then we compare the PCA visualization results of $\boldsymbol{v}_2\sim\boldsymbol{v}_4$ and $\boldsymbol{v}_2^{'}\sim\boldsymbol{v}_4^{'}$ respectively. 
Fig.\ref{fig:pca_4}(a)(b)(c) illustrate the visualization results of $\boldsymbol{v}_2^{'}\sim\boldsymbol{v}_4^{'}$, and Fig.\ref{fig:pca_4}(d)(e)(f) illustrate the visualization results of $\boldsymbol{v}_2\sim\boldsymbol{v}_4$. 
The $\boldsymbol{v}_2, \boldsymbol{v}_3, \boldsymbol{v}_4$ belonging to groups without modifying eyes such as G1, G2, G3, G4, G8, G9, G10, G11, have a bigger in-class distance compared with $\boldsymbol{v}_2^{'}, \boldsymbol{v}_3^{'}, \boldsymbol{v}_4^{'}$. 
This observation exactly demonstrates the removal of the first two instance normalization layers in pix2pixHD generator can better extract the low-level features of the input sketch. 
The improvement on generator network can effectively alleviate the issue that changing one part of the sketch influence the other parts of the generated image. So our designed network can suport fine-grained control on the generated images and enhance texture details generation.
\begin{figure*}[htb]
	\centering
	\includegraphics[width=0.8 \textwidth]{pca_4.png}
	\caption{Comparative PCA visualization results of $\boldsymbol{v}_2\sim\boldsymbol{v}_4$ and $\boldsymbol{v}_2^{'}\sim\boldsymbol{v}_4^{'}$}
	\label{fig:pca_4}
\end{figure*}


\section{Experiments on Sketch-to-Image}
We conduct extensive experiments to evaluate our proposed method on sketch-based face image generation.


%In Sec.~\ref{sec:datasets}, we introduce how to produce our training and testing datasets(). 
%In Sec.~\ref{sec:augmentation}, we introduce the method of data augmentation aiming to alleviate the issue that the generated image has poor tolerance to the spatial position change of the input sketches. And we show the generation results of the model trained with augmented data. 
%In Sec.~\ref{sec:results}, we conduct comparative experiments on hand-drawn sketches between our methods and pix2pixHD baseline. We show that our method achieves a better fine-grained control on generated images.

\paragraph{Datasets.}
\nt{CelebA-HQ dataset contains more than 30,000 face images generated by PGGAN~\cite{pggan} of resolution 1024×1024. } Since the images are cropped according to face landmarks, each image is global aligned.
We extracted 68 landmarks from the face images in the CelebA-HQ dataset, and then connected these points in sequence with a line of pixel 2 to form the contour. As the face photos in Celeba-HQ dataset are global aligned, so are the contours. Contour is more simple and clean than other types of sketch like edge maps\cite{csagan} and mask edge maps\cite{maskgan}, so it is more suitble to imitate human hand-drawn sketch. Therefore, it is more reasonable to use contour as training data.
For the purpose of the experiment, we scaled the picture of resolution 512×512. After selection, the training set contains 14,973 pairs of contour and face photos, while the test set contains 4,992 pairs of contour and face photos. 
In order to evaluate the generalization ability of our model on the hand-drawn sketches, we develop an interactive interface for drawing sketches and displaying the generated photos in real time.

\subsection{Data Augmentation}\label{sec:augmentation} 
Since the face photos of the CalebA-HQ dataset are cropped with the reference of facial landmarks, and our training data contours are also obtained based on facial landmarks, we intuitively believe that all the facial landmarks of the training data have spatial consistency. 
To verify the hypothesis above, we calculate the average face of all the training data, as shown in Fig.\ref{fig:average_face}. We find that the facial features and facial contours of training data are basically in the same position, in other words, the training data is global aligned. This issue leads to degraded generalization ability of the model. 
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.18 \textwidth]{average_face.png}
	\caption{The average face of all the training data. }
	\label{fig:average_face}
\end{figure}
In order to imitate the human hand-drawn sketches, we apply random translation and rotation to the training contours. Specifically, offsets randomly selected from $[-d,d]^2$ and angles randomly selected from $[-\theta,\theta]$ are added to the training contours, where $d$ is the maximum offset and $\theta$ is the maximum angle and we set $d=25$, $\theta=7^\circ$ in our experiments. 
But face photos are not translated or rotated because we expect the generated images to remain global aligned regardless of the spatial location of the input sketches.
Fig.\ref{fig:data_augmentation} illustrates the comparisons between images generated before data augmentation and  that after data augmentation. 
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.35 \textwidth]{data_augmentation.png}
	\caption{Comparison between images generated before and after data augmentation. The quality of generated images after data augmentation is better than that before data augmentation when the inputs deviate from standard position.}
	\label{fig:data_augmentation}
\end{figure}

\subsection{Qualitatively Comparisons}\label{sec:results}
As you can see in Fig.\ref{fig:ablation}, our model generates the most photorealistic images in contrast to the $M_2$ model producing images with lowest sense of reality. It indicates that removing too many instance normalization layers in generator can weaken the model as reason of gradient vanishing.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.5 \textwidth]{ablation.png}
	\caption{Comparison on amount of instance normalization layers. We refer to original pix2pixHD model as $M_1$, refer to the model of which generator gets rid of first 5 IN layers as $M_2$.}
	\label{fig:ablation}
\end{figure}  

We perform comparative experiments between our model and pix2pixHD baseline on hand-drawn sketches.
Results shown in Fig.\ref{fig:compare_1} demonstrate that the baseline model frequently fails to produce realistic textures. In contrast, our results are more realistic with fine textures.  
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.45 \textwidth]{texture.png}
	\caption{Magnified details of images generated by our model and baseline model. The top row shows that the textures of teeth generated by pix2pixHD baseline are blurry, while our results have clear textures of teeth. The bottom row shows that some chaotic noises often emerge in the forehead of images generated by pix2pixHD baseline, but our results do not have this issue.}
	\label{fig:compare_1}
\end{figure}

Our model can achieve fine-grained control on generated images, whereas baseline model cannot. When we modify the lines of face parts or face shapes of input freehand sketches, the corresponding parts of images generated by our model change simultaneously but other areas remain unchanged. 
In comparison, modifying strokes of sketches influences not only the content in corresponding areas of images generated by baseline model but also the content in other areas. 
Fig.\ref{fig:compare_2} and Fig.\ref{fig:compare_3}~ show several face images generated by our model and baseline model when changing lines of mouth and nose separately in sketches.
The results shown in row 1, 2 of Fig.\ref{fig:compare_2} demonstrate that the images generated by our model change obviously in mouth shape and preserve structure conformance with input sketches when modifying the lines of mouth in sketches. But the results generated by baseline model do not have an obvious change in mouth shape.
The results shown in Fig.\ref{fig:compare_3} illustrate that the images generated by our model remain unchanged in other areas especially in eyes when altering the shapes of nose in sketches, but the images generated by baseline model change obviously in eyes direction. And as shown in row 3 of Fig.\ref{fig:compare_2}, the generation results of our model do not change except the mouth when modifying the mouth shape in sketches, while the generation quality of baseline model is degraded vastly especially in the eye area. 
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.4 \textwidth]{mouth_editing.png}
	\caption{Comparison between our model and baseline model tested with mouth-altered sketches. }
	\label{fig:compare_2}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.45 \textwidth]{nose_editing.png}
	\caption{Comparison between our model and baseline model tested with nose-altered sketches.}
	\label{fig:compare_3}
\end{figure}

\section{Conclusion}
In this paper, we utilize PCA, a visualization approach to analyze the feature embedding for sketches with a group of specific changes.  
Then we modify the baseline image translation model by removing the first two instance normalization layers in its generator. 
We show the effectiveness of the proposed simple modification in the generator architecture on making a significant improvement of the image generation quality and conformance with user intention. 
User studies demonstrate that our method surpass the baseline method regarding both face editing and image fidelity.    


%\section{About the paper submission}
%
%\subsection{Paper length}
%cvm papers may be between 4 pages and 14 pages. Over length papers will simply not be reviewed.
%
%%-------------------------------------------------------------------------
%\subsection{Draft and final copy}
%The \LaTeX\ style defines a printed ruler which should be present in the
%version submitted for review.  The ruler is provided in order that
%reviewers may comment on particular lines in the paper without
%circumlocution. The camera ready copy should not contain a ruler.
%(\LaTeX\ users may uncomment the \verb'\cvmfinalcopy' command in the document preamble.)
%
%\subsection{Blind review}
%Many authors misunderstand the concept of anonymizing for blind
%review.  Blind review does not mean that one must remove
%citations to one's own work---in fact it is often impossible to
%review a paper unless the previous citations are known and
%available.
%Blind review means that you do not use the words ``my'' or ``our''
%when citing previous work.
%
%%\begin{figure}[t]
%%\begin{center}
%%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%%   \includegraphics[width=0.8\linewidth]{pix2pixhd_D.png}
%%\end{center}
%%   \caption{Example of caption. }
%%\label{fig:long}
%%\label{fig:onecol}
%%\end{figure}
%
%\subsection{Miscellaneous}
%
%\noindent
%Compare the following:\\
%\begin{tabular}{ll}
% \verb'$conf_a$' &  $conf_a$ \\
% \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
%\end{tabular}\\
%See The \TeX book, p165.
%
%The space after \eg, meaning ``for example'', should not be a
%sentence-ending space. So \eg is correct, {\em e.g.} is not.  The provided
%\verb'\eg' macro takes care of this.
%
%When citing a multi-author paper, you may save space by using ``et alia'',
%shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word.)
%However, use it only when there are three or more authors.  Thus, the
%following is correct: ``
%   Frobnication has been trendy lately.
%   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''
%
%This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...''
%because reference~\cite{Alpher03} has just two authors.  If you use the
%\verb'\etal' macro provided, then you need not worry about double periods
%when used at the end of a sentence as in Alpher \etal.
%
%For this citation style, keep multiple citations in numerical (not
%chronological) order, so prefer \cite{Alpher03,Alpher02,Authors12} to
%\cite{Alpher02,Alpher03,Authors12}.
%
%%-------------------------------------------------------------------------
%\subsection{References}
%
%List and number all bibliographical references in 9-point Times,
%single-spaced, at the end of your paper. When referenced in the text,
%enclose the citation number in square brackets, for
%example~\cite{Authors12}.  Where appropriate, include the name(s) of
%editors of referenced books.
%
%\begin{table}
%\begin{center}
%\begin{tabular}{|l|c|}
%\hline
%Name & Performance \\
%\hline\hline
%A & OK\\
%B & Bad \\
%Ours & Great\\
%\hline
%\end{tabular}
%\end{center}
%\caption{An example for using tables.}
%\end{table}
%
%%-------------------------------------------------------------------------
%\subsection{Illustrations, graphs, and photographs}
%
%All graphics should be centered.  Please ensure that any point you wish to
%make is resolvable in a printed copy of the paper.  Resize fonts in figures
%to match the font in the body text, and choose line widths which render
%effectively in print.  Many readers (and reviewers), even of an electronic
%copy, will choose to print your paper in order to read it.  You cannot
%insist that they do otherwise, and therefore must not assume that they can
%zoom in to see tiny details on a graphic.
%
%When placing figures in \LaTeX, it's almost always best to use
%\verb+\includegraphics+, and to specify the  figure width as a multiple of
%the line width as in the example below
%{\small\begin{verbatim}
%   \usepackage[dvips]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]
%                   {myfile.eps}
%\end{verbatim}
%}
%
%
%%-------------------------------------------------------------------------
%
{\small
\bibliographystyle{cvm}
\bibliography{cvmbib}
}

\end{document}
